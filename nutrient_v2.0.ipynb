{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's study LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.schema import Document\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain.document_loaders import PyMuPDFLoader,ArxivLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain.document_transformers import LongContextReorder\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "from langchain.schema.runnable.passthrough import RunnableAssign\n",
    "from operator import itemgetter\n",
    "\n",
    "from faiss import IndexFlatL2\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "from functools import partial\n",
    "from rich.console import Console\n",
    "from rich.style import Style\n",
    "from rich.theme import Theme\n",
    "\n",
    "\n",
    "console = Console()\n",
    "base_style = Style(color=\"#76B900\", bold=True)\n",
    "pprint = partial(console.print, style=base_style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you plan to use Gemini, can get the API [here](https://ai.google.dev/gemini-api/docs/api-key#windows). Click the `Get gemini API key in Google AI Studio`.\n",
    "\n",
    "Once you have it, uncomment and run the cell below, then paste the API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can \n",
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: \")\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eka Sulistyawan\\AppData\\Local\\Temp\\ipykernel_17816\\1023208638.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Eka Sulistyawan\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# in the nvidia study exercise, they use \n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "# llm = OllamaLLM(model=\"llama3.1:8b\") # uncomment this if you wanna use OllamaLLM locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create vector stores from nutrient papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Documents:\n",
      " - Within-person comparison of eating behaviors, time of eating, and dietary intake on days with and without breakfast: NHANES 2005–20101–3\n",
      " - Nutrient Intakes from Meals and Snacks Differ with Age in Middle-Aged and Older Americans\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "docs = []\n",
    "for fname in os.listdir(\"./PAPER_DOCS/\"):\n",
    "    loader = PyMuPDFLoader(f\"./PAPER_DOCS/{fname}\")\n",
    "    docs.append(loader.load())\n",
    "\n",
    "for doc in docs:\n",
    "    content = json.dumps(doc[0].page_content)\n",
    "    if \"References\" in content:\n",
    "        doc[0].page_content = content[:content.index(\"References\")]\n",
    "\n",
    "# print(\"Chunking Documents\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \";\", \",\", \" \"],\n",
    ")\n",
    "docs_chunks = [text_splitter.split_documents(doc) for doc in docs]\n",
    "docs_chunks = [[c for c in dchunks if len(c.page_content) > 200] for dchunks in docs_chunks]\n",
    "\n",
    "# ## Make some custom Chunks to give big-picture details\n",
    "doc_string = \"Available Documents:\"\n",
    "doc_metadata = []\n",
    "for chunks in docs_chunks:\n",
    "    metadata = getattr(chunks[0], 'metadata', {})\n",
    "    doc_string += \"\\n - \" + metadata['title']\n",
    "    doc_metadata += [str(metadata)]\n",
    "\n",
    "extra_chunks = [doc_string] + doc_metadata\n",
    "print(doc_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing Vector Stores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Eka Sulistyawan\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 484 ms\n",
      "Wall time: 1.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"Constructing Vector Stores\")\n",
    "vecstores = [FAISS.from_texts(extra_chunks, embeddings)]\n",
    "vecstores += [FAISS.from_documents(doc_chunks, embeddings) for doc_chunks in docs_chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructed aggregate docstore with 115 chunks\n"
     ]
    }
   ],
   "source": [
    "embed_dims = len(embeddings.embed_query(\"test\"))\n",
    "def default_FAISS():\n",
    "    '''Useful utility for making an empty FAISS vectorstore'''\n",
    "    return FAISS(\n",
    "        embedding_function=embeddings,\n",
    "        index=IndexFlatL2(embed_dims),\n",
    "        docstore=InMemoryDocstore(),\n",
    "        index_to_docstore_id={},\n",
    "        normalize_L2=False\n",
    "    )\n",
    "\n",
    "def aggregate_vstores(vectorstores):\n",
    "    ## Initialize an empty FAISS Index and merge others into it\n",
    "    ## We'll use default_faiss for simplicity, though it's tied to your embedder by reference\n",
    "    agg_vstore = default_FAISS()\n",
    "    for vstore in vectorstores:\n",
    "        agg_vstore.merge_from(vstore)\n",
    "    return agg_vstore\n",
    "\n",
    "## Unintuitive optimization; merge_from seems to optimize constituent vector stores away\n",
    "docstore = aggregate_vstores(vecstores)\n",
    "\n",
    "print(f\"Constructed aggregate docstore with {len(docstore.docstore._dict)} chunks\")\n",
    "docstore.save_local(\"docstore_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Price Tags and menu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example tabular data\n",
    "table_data = [\n",
    "    {\"Product Name\": \"Chicken set\", \"Price\": \"$10\"},\n",
    "    {\"Product Name\": \"Pork set\", \"Price\": \"$15\"},\n",
    "    {\"Product Name\": \"Instant noodle\", \"Price\": \"$25\"},\n",
    "]\n",
    "\n",
    "# Concatenate fields into a single string representation for each row\n",
    "row_texts = [\n",
    "    f\"{row['Product Name']} {row['Price']}\"\n",
    "    for row in table_data\n",
    "]\n",
    "\n",
    "menus = FAISS.from_texts(row_texts, embedding=embeddings)\n",
    "\n",
    "\n",
    "def docs2str(docs, title=\"Document\"):\n",
    "    \"\"\"Useful utility for making chunks into context string. Optional, but useful\"\"\"\n",
    "    out_str = \"\"\n",
    "    for doc in docs:\n",
    "        doc_name = getattr(doc, 'metadata', {}).get('Title', title)\n",
    "        if doc_name:\n",
    "            out_str += f\"[Quote from {doc_name}] \"\n",
    "        out_str += getattr(doc, 'page_content', str(doc)) + \"\\n\"\n",
    "    return out_str\n",
    "\n",
    "## Optional; Reorders longer documents to center of output text\n",
    "long_reorder = RunnableLambda(LongContextReorder().transform_documents)\n",
    "\n",
    "context_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Answer the question using only the context\"\n",
    "    \"\\n\\nRetrieved Context: {context}\"\n",
    "    \"\\n\\nUser Question: {question}\"\n",
    "    \"\\nAnswer the user conversationally. User is not aware of context.\"\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        'context': menus.as_retriever() | long_reorder | docs2str,\n",
    "        'question': (lambda x:x)\n",
    "    }\n",
    "    | context_prompt\n",
    "    # | RPrint()\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Historic Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docs2str(docs, title=\"Document\"):\n",
    "    \"\"\"Useful utility for making chunks into context string. Optional, but useful\"\"\"\n",
    "    out_str = \"\"\n",
    "    for doc in docs:\n",
    "        doc_name = getattr(doc, 'metadata', {}).get('Title', title)\n",
    "        if doc_name:\n",
    "            out_str += f\"[Quote from {doc_name}] \"\n",
    "        out_str += getattr(doc, 'page_content', str(doc)) + \"\\n\"\n",
    "    return out_str\n",
    "\n",
    "def save_memory_and_get_output(d, vstore):\n",
    "    \"\"\"Accepts 'input'/'output' dictionary and saves to convstore\"\"\"\n",
    "    vstore.add_texts([f\"User said {d.get('input')}\", f\"Agent said {d.get('output')}\"])\n",
    "    return d.get('output')\n",
    "\n",
    "## Optional; Reorders longer documents to center of output text\n",
    "long_reorder = RunnableLambda(LongContextReorder().transform_documents)\n",
    "\n",
    "memorized_conv = [\"User not saying anything\",\"Agent said Hi! what could I help?\"]\n",
    "convstore = FAISS.from_texts(memorized_conv, embedding=embeddings)\n",
    "retriever = convstore.as_retriever()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purchase History "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "csv_file = './your_data/meal_history.csv'\n",
    "\n",
    "# Function to log a purchase with a timestamp\n",
    "\n",
    "def extract_meal_details(response):\n",
    "    # Regular expression to match each meal item and its price\n",
    "    pattern = r\"\\[\\s*([^,]+)\\s*,\\s*\\$(\\d+(\\.\\d{1,2})?)\\s*\\]\"\n",
    "    \n",
    "    meals = []\n",
    "    \n",
    "    # Find all matches using the regex pattern\n",
    "    matches = re.findall(pattern, response)\n",
    "    \n",
    "    for match in matches:\n",
    "        item = match[0].strip()  # Meal name\n",
    "        price = float(match[1])  # Price as float\n",
    "        meals.append({\"item\": item, \"price\": price})\n",
    "    \n",
    "    return meals\n",
    "\n",
    "def load_purchase_history():\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        row_texts = df.apply(lambda row: f\"{row['Timestamp']} - {row['Item']} for ${row['Price']}\", axis=1).tolist()\n",
    "        return row_texts\n",
    "    except FileNotFoundError:\n",
    "        return []\n",
    "\n",
    "# Function to log purchase to CSV\n",
    "def log_purchase(meals):\n",
    "    \"\"\"Logs the confirmed purchase to the CSV file.\"\"\"\n",
    "    # Get the current timestamp\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    # Create a DataFrame for the meal details\n",
    "    meal_entries = []\n",
    "    \n",
    "    for meal in meals:\n",
    "        # For each meal, add a timestamp and log it\n",
    "        print(meal['item'])\n",
    "        meal_entries.append({\n",
    "            \"Timestamp\": timestamp,\n",
    "            \"Item\": meal[\"item\"],\n",
    "            \"Price\": meal[\"price\"]\n",
    "        })\n",
    "    \n",
    "    # Create a DataFrame or load existing CSV\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file)\n",
    "    except FileNotFoundError:\n",
    "        df = pd.DataFrame(columns=[\"Timestamp\", \"Item\", \"Price\"])\n",
    "\n",
    "    # Append new entries\n",
    "    df = pd.concat([df, pd.DataFrame(meal_entries)], ignore_index=True)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(csv_file, index=False)\n",
    "    print(f\"Logged {len(meal_entries)} meal(s) to {csv_file}.\")\n",
    "\n",
    "# Inside chat_gen after response\n",
    "purchase_history = FAISS.from_texts(load_purchase_history(),embedding=embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = \"I buy these [chicken set, $10], [instant noodle, $25], [pork set, $15]\"\n",
    "\n",
    "# meal_details = extract_meal_details(response)\n",
    "# log_purchase(meal_details)\n",
    "# # meal_details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nutrient's related RAG\n",
    "\n",
    "I want the model to?\n",
    "1. Answer based on the current meal set provided\n",
    "2. Able to recommend menu depending on budget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RPrint(preface=\"\"):\n",
    "    \"\"\"Simple passthrough \"prints, then returns\" chain\"\"\"\n",
    "    def print_and_return(x, preface):\n",
    "        if preface: print(preface, end=\"\")\n",
    "        pprint(x)\n",
    "        return x\n",
    "    return RunnableLambda(partial(print_and_return, preface=preface))\n",
    "\n",
    "def docs2str(docs, title=\"Document\"):\n",
    "    \"\"\"Useful utility for making chunks into context string. Optional, but useful\"\"\"\n",
    "    out_str = \"\"\n",
    "    for doc in docs:\n",
    "        doc_name = getattr(doc, 'metadata', {}).get('Title', title)\n",
    "        if doc_name:\n",
    "            out_str += f\"[Quote from {doc_name}] \"\n",
    "        out_str += getattr(doc, 'page_content', str(doc)) + \"\\n\"\n",
    "    return out_str\n",
    "long_reorder = RunnableLambda(LongContextReorder().transform_documents)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([(\"system\",\n",
    "    \"You are a document chatbot. Help the user as they ask questions about documents.\"\n",
    "    \" User messaged just asked: {input}\\n\\n\"\n",
    "    \" If the user intended to purchase meal, summarize the meal and present in list format: \"\n",
    "    \" [item name,price],[item 2 name, price], ... \\n\\n\"\n",
    "    \" Once confirmed, say meal purchased succesfully and continue\\n\\n\"\n",
    "    \" Otherwise,\"\n",
    "    \" From this, we have retrieved the following potentially-useful info: \"\n",
    "    \" These items is sold by the cafeteria: \\n{list_of_items}\\n\\n\"\n",
    "    \" Document Retrieval:\\n{context}\\n\\n\"\n",
    "    \" Previous conversation:\\n{memory}\\n\\n\"\n",
    "    \" Previous meal purchaed:\\n{purchased_meal}\\n\\n\"\n",
    "    \" (Answer only from retrieval. Only cite sources that are used. Make your response conversational.)\"\n",
    "    \" (Answer briefly.)\"\n",
    "), ('user', '{input}')])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### add second agent to deal with data?\n",
    "# why we need to add data-reasoning agent? We hope by adding this, the model may \n",
    "data_agent_prompt = ChatPromptTemplate.from_messages([(\"system\",\n",
    "    \"You are a chatbot that understands how to deal with data.\\n\\n\"\n",
    "    \"User just messaged with {input}\\n\\n\"\n",
    "    \"Identify if user had asked question related to statistics.\\n\\n\"\n",
    "    \"Respond with one keyword of the statistical measure with the following format\\n\\n\"\n",
    "    \"<keyword of statistical measure>\\n\\n\"\n",
    "    \"You can only work on the following statistica measure:\\n\\n\"\n",
    "    \"mean, linear regression, sum\\n\\n\"\n",
    "    \"If there is no intention to do statistics, answer with <no>\\n\\n\"\n",
    "    ),(\"user\",\"{input}\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "retrieval_chain = (\n",
    "    {'input' : (lambda x: x)}\n",
    "    | RunnableAssign({'list_of_items' : itemgetter('input') | menus.as_retriever()  | long_reorder | docs2str})\n",
    "    | RunnableAssign({'context' : itemgetter('input') | docstore.as_retriever()  | long_reorder | docs2str})\n",
    "    | RunnableAssign({'memory' : itemgetter('input') | convstore.as_retriever()  | long_reorder | docs2str})\n",
    "    | RunnableAssign({'purchased_meal': itemgetter('input') | purchase_history.as_retriever() | long_reorder | docs2str})  # Add purchase history retrieval\n",
    ")\n",
    "\n",
    "stream_chain = (\n",
    "    chat_prompt | llm | StrOutputParser()\n",
    "    )\n",
    "\n",
    "data_agent_chain = (\n",
    "    data_agent_prompt | llm | StrOutputParser()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<no>'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_agent_chain.invoke(\"What should I buy next?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # method 1\n",
    "# # method 1, no saved conversation\n",
    "# def chat_gen(message, history=[], return_buffer=True):\n",
    "#     buffer = \"\"\n",
    "#     ## First perform the retrieval based on the input message\n",
    "#     retrieval = retrieval_chain.invoke(message)\n",
    "#     meal_purchases = []\n",
    "\n",
    "#     ## Then, stream the results of the stream_chain\n",
    "#     for token in stream_chain.stream(retrieval):\n",
    "#         buffer += token\n",
    "#         # Check if we detect the \"meal purchased successfully\" confirmation\n",
    "#         if \"meal purchased successfully\" in buffer.lower():\n",
    "#             # Assuming extract_meal_details is defined to extract meal item and price from response\n",
    "#             meal_item, meal_price = extract_meal_details(buffer)\n",
    "            \n",
    "#             if meal_item and meal_price:\n",
    "#                 meal_purchases.append({\"item\": meal_item, \"price\": meal_price})\n",
    "#                 log_purchase(meal_item, meal_price)  # Log the purchase to CSV\n",
    "#                 buffer = \"\"  # Reset buffer after logging\n",
    "#         ## If you're using standard print, keep line from getting too long\n",
    "#         # yield buffer if return_buffer else token\n",
    "#         yield token\n",
    "# # method 2: with saved conversation\n",
    "# RAGChain = (\n",
    "#     {\n",
    "#         'memory': convstore.as_retriever() | long_reorder | docs2str,\n",
    "#         'list_of_items': menu_retriever | long_reorder | docs2str,\n",
    "#         'context': docstore.as_retriever() | long_reorder | docs2str,\n",
    "#         'input': (lambda x:x)\n",
    "#     }\n",
    "#     | RunnableAssign({'output' : chat_prompt | llm | StrOutputParser()})\n",
    "#     | partial(save_memory_and_get_output, vstore=convstore)\n",
    "# )\n",
    "\n",
    "# RAGChain.invoke(\"Hi?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Eka Sulistyawan\\anaconda3\\Lib\\site-packages\\gradio\\components\\chatbot.py:279: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://84732926cc9e89a3c6.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://84732926cc9e89a3c6.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pork set\n",
      "instant noodle\n",
      "chicken set\n",
      "Logged 3 meal(s) to ./your_data/meal_history.csv.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "# implement chat-like Gradio\n",
    "def save_memory_and_get_output(d, vstore):\n",
    "    \"\"\"Accepts 'input'/'output' dictionary and saves to convstore\"\"\"\n",
    "    vstore.add_texts([\n",
    "        f\"User previously responded with {d.get('input')}\",\n",
    "        f\"Agent previously responded with {d.get('output')}\"\n",
    "    ])\n",
    "    return d.get('output')\n",
    "\n",
    "def chat_gen(message, history=[], return_buffer=True):\n",
    "    buffer = \"\"\n",
    "    meal_purchases = []\n",
    "    ## First perform the retrieval based on the input message\n",
    "    retrieval = retrieval_chain.invoke(message)\n",
    "    line_buffer = \"\"\n",
    "\n",
    "    ## Then, stream the results of the stream_chain\n",
    "    for token in stream_chain.stream(retrieval):\n",
    "        buffer += token\n",
    "        ## If you're using standard print, keep line from getting too long\n",
    "        # Check if we detect the \"meal purchased successfully\" confirmation\n",
    "        if \"meal purchased successfully\" in buffer.lower():\n",
    "            # Assuming extract_meal_details is defined to extract meal item and price from response\n",
    "            meal_details = extract_meal_details(buffer.lower())\n",
    "            log_purchase(meal_details)\n",
    "            \n",
    "        yield buffer if return_buffer else token\n",
    "\n",
    "    ## Lastly, save the chat exchange to the conversation memory buffer\n",
    "    save_memory_and_get_output({'input':  message, 'output': buffer}, convstore)\n",
    "\n",
    "\n",
    "# chatbot = gr.Chatbot(value = [[None, initial_msg]])\n",
    "demo = gr.ChatInterface(chat_gen).queue()\n",
    "\n",
    "try:\n",
    "    demo.launch(debug=True, share=False, show_api=False)\n",
    "    demo.close()\n",
    "except Exception as e:\n",
    "    demo.close()\n",
    "    print(e)\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
